{"cells":[{"cell_type":"markdown","metadata":{},"source":["# E-commerce : boosting online sales with NLP !\n","\n","## Introduction :\n","\n","- In this project, I work with a corpus of item descriptions from an outdoor apparel brand's product catalog.\n","\n","- The main goal is to use some NLP techniques to analyze text description of the product catalog in order to identify similar product, build a recommender system and create new topics with more meaning."]},{"cell_type":"markdown","metadata":{},"source":["### Table of Contents\n","\n","* [Part 1 : Preprocessing](#chapter1)\n","    * [Part 1.1 : Libraries & data loading](#section_1_1)\n","    * [Part 1.2 : Text preprocessings](#section_1_2)\n","    \n","\n","* [Part 2 : Clustering model --> Identify similar products](#chapter2)\n","\n","* [Part 3 : Recommender system](#chapter3)\n","\n","* [Part 4 : LSA model --> topics extraction](#chapter4)\n"]},{"cell_type":"markdown","metadata":{},"source":["Pre-requisite : if necessary, install the required libraries for NLP (spacy, wordclouds) and download the english language model. Then, import all the libraries."]},{"cell_type":"markdown","metadata":{},"source":["## Part 1 : Preprocessing <a class=\"anchor\" id=\"chapter1\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["### Part 1.1 : Libraries & data loading <a class=\"anchor\" id=\"section_1_1\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["Install libraries & load data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:22:37.076517Z","iopub.status.busy":"2021-10-10T09:22:37.076147Z","iopub.status.idle":"2021-10-10T09:22:46.332232Z","shell.execute_reply":"2021-10-10T09:22:46.331072Z","shell.execute_reply.started":"2021-10-10T09:22:37.076425Z"},"trusted":true},"outputs":[],"source":["# Install spacy quietly\n","!pip install spacy -q"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:22:46.334714Z","iopub.status.busy":"2021-10-10T09:22:46.334438Z","iopub.status.idle":"2021-10-10T09:22:57.548443Z","shell.execute_reply":"2021-10-10T09:22:57.547413Z","shell.execute_reply.started":"2021-10-10T09:22:46.334681Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n","full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["# Download english language model\n","!python -m spacy download en -q"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:22:57.552387Z","iopub.status.busy":"2021-10-10T09:22:57.55214Z","iopub.status.idle":"2021-10-10T09:23:05.363796Z","shell.execute_reply":"2021-10-10T09:23:05.363109Z","shell.execute_reply.started":"2021-10-10T09:22:57.55236Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wordcloud in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.9.3)\n","Requirement already satisfied: numpy>=1.6.1 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wordcloud) (1.26.1)\n","Requirement already satisfied: pillow in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wordcloud) (10.2.0)\n","Requirement already satisfied: matplotlib in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wordcloud) (3.8.1)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->wordcloud) (2.8.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\ishs4\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"]}],"source":["# Install wordcloud\n","!pip install wordcloud"]},{"cell_type":"markdown","metadata":{},"source":["1. Import pandas, numpy, spacy, sklearn (tfidf vectorizer, DBSCAN and TruncatedSVD), matplotlib and wordcloud"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:24:28.117861Z","iopub.status.busy":"2021-10-10T09:24:28.117157Z","iopub.status.idle":"2021-10-10T09:24:29.273688Z","shell.execute_reply":"2021-10-10T09:24:29.273063Z","shell.execute_reply.started":"2021-10-10T09:24:28.11781Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import spacy\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import DBSCAN\n","from sklearn.decomposition import TruncatedSVD\n","\n","import matplotlib.pyplot as plt\n","import wordcloud"]},{"cell_type":"markdown","metadata":{},"source":["2. Reading the corpus and put it in a DataFrame named `corpus`. I print the full description in the first line"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:24:42.752188Z","iopub.status.busy":"2021-10-10T09:24:42.751241Z","iopub.status.idle":"2021-10-10T09:24:42.781306Z","shell.execute_reply":"2021-10-10T09:24:42.780293Z","shell.execute_reply.started":"2021-10-10T09:24:42.752145Z"},"trusted":true},"outputs":[],"source":["corpus = pd.read_csv('C:/Users/ishs4/Desktop/promotheo/sample-data.csv')\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:24:44.930962Z","iopub.status.busy":"2021-10-10T09:24:44.930662Z","iopub.status.idle":"2021-10-10T09:24:44.951235Z","shell.execute_reply":"2021-10-10T09:24:44.950315Z","shell.execute_reply.started":"2021-10-10T09:24:44.93093Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(500, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Active classic boxers - There's a reason why o...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Active sport briefs - These superbreathable no...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                        description\n","0   1  Active classic boxers - There's a reason why o...\n","1   2  Active sport boxer briefs - Skinning up Glory ...\n","2   3  Active sport briefs - These superbreathable no...\n","3   4  Alpine guide pants - Skin in, climb ice, switc...\n","4   5  Alpine wind jkt - On high ridges, steep ice an..."]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["print(corpus.shape)\n","corpus.head()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:24:55.080695Z","iopub.status.busy":"2021-10-10T09:24:55.080393Z","iopub.status.idle":"2021-10-10T09:24:55.092204Z","shell.execute_reply":"2021-10-10T09:24:55.091484Z","shell.execute_reply.started":"2021-10-10T09:24:55.080662Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Description of the first product :  Active classic boxers - There's a reason why our boxers are a cult favorite - they keep their cool, especially in sticky situations. The quick-drying, lightweight underwear takes up minimal space in a travel pack. An exposed, brushed waistband offers next-to-skin softness, five-panel construction with a traditional boxer back for a classic fit, and a functional fly. Made of 3.7-oz 100% recycled polyester with moisture-wicking performance. Inseam (size M) is 4 1/2\". Recyclable through the Common Threads Recycling Program.<br><br><b>Details:</b><ul> <li>\"Silky Capilene 1 fabric is ultralight, breathable and quick-to-dry\"</li> <li>\"Exposed, brushed elastic waistband for comfort\"</li> <li>5-panel construction with traditional boxer back</li> <li>\"Inseam (size M) is 4 1/2\"\"\"</li></ul><br><br><b>Fabric: </b>3.7-oz 100% all-recycled polyester with Gladiodor natural odor control for the garment. Recyclable through the Common Threads Recycling Program<br><br><b>Weight: </b>99 g (3.5 oz)<br><br>Made in Mexico.\n"]}],"source":["print('Description of the first product : ',corpus.loc[0,'description'])"]},{"cell_type":"markdown","metadata":{},"source":["`Description is dirty due to HTML elements, punctuation and the not standardization of the words.`"]},{"cell_type":"markdown","metadata":{},"source":["### Part 1.2 : Text preprocessings <a class=\"anchor\" id=\"section_1_2\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["3. Using `str` methods to clean the texts. I save the clean texts into a column named `clean_description`"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:24:55.094768Z","iopub.status.busy":"2021-10-10T09:24:55.093734Z","iopub.status.idle":"2021-10-10T09:24:55.16291Z","shell.execute_reply":"2021-10-10T09:24:55.161997Z","shell.execute_reply.started":"2021-10-10T09:24:55.094718Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Description cleaned of the first product :  Active classic boxers - There's a reason why our boxers are a cult favorite - they keep their cool, especially in sticky situations. The quick-drying, lightweight underwear takes up minimal space in a travel pack. An exposed, brushed waistband offers next-to-skin softness, five-panel construction with a traditional boxer back for a classic fit, and a functional fly. Made of 3.7-oz 100% recycled polyester with moisture-wicking performance. Inseam (size M) is 4 1/2\". Recyclable through the Common Threads Recycling Program.<br><br><b>Details:</b><ul> <li>\"Silky Capilene 1 fabric is ultralight, breathable and quick-to-dry\"</li> <li>\"Exposed, brushed elastic waistband for comfort\"</li> <li>5-panel construction with traditional boxer back</li> <li>\"Inseam (size M) is 4 1/2\"\"\"</li></ul><br><br><b>Fabric: </b>3.7-oz 100% all-recycled polyester with Gladiodor natural odor control for the garment. Recyclable through the Common Threads Recycling Program<br><br><b>Weight: </b>99 g (3.5 oz)<br><br>Made in Mexico.\n"]}],"source":["# Remove HTML elements\n","corpus['clean_description'] = corpus['description'].str.replace(r\"<[a-z/]+>\", \" \") \n","# Remove special characters and numbers\n","corpus['clean_description'] = corpus['clean_description'].str.replace(r\"[^A-Za-z]+\", \" \") \n","print('Description cleaned of the first product : ',corpus.loc[0,'clean_description'])"]},{"cell_type":"markdown","metadata":{},"source":["4. Tranforming every character into lowercase."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:24:55.164801Z","iopub.status.busy":"2021-10-10T09:24:55.164501Z","iopub.status.idle":"2021-10-10T09:24:55.172988Z","shell.execute_reply":"2021-10-10T09:24:55.172374Z","shell.execute_reply.started":"2021-10-10T09:24:55.16476Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Description in lower case of the first product :  active classic boxers - there's a reason why our boxers are a cult favorite - they keep their cool, especially in sticky situations. the quick-drying, lightweight underwear takes up minimal space in a travel pack. an exposed, brushed waistband offers next-to-skin softness, five-panel construction with a traditional boxer back for a classic fit, and a functional fly. made of 3.7-oz 100% recycled polyester with moisture-wicking performance. inseam (size m) is 4 1/2\". recyclable through the common threads recycling program.<br><br><b>details:</b><ul> <li>\"silky capilene 1 fabric is ultralight, breathable and quick-to-dry\"</li> <li>\"exposed, brushed elastic waistband for comfort\"</li> <li>5-panel construction with traditional boxer back</li> <li>\"inseam (size m) is 4 1/2\"\"\"</li></ul><br><br><b>fabric: </b>3.7-oz 100% all-recycled polyester with gladiodor natural odor control for the garment. recyclable through the common threads recycling program<br><br><b>weight: </b>99 g (3.5 oz)<br><br>made in mexico.\n"]}],"source":["# Lowercase\n","corpus['clean_description'] = corpus['clean_description'].str.lower()\n","print('Description in lower case of the first product : ',corpus.loc[0,'clean_description'])"]},{"cell_type":"markdown","metadata":{},"source":["5. Using spacy to tokenize the documents and put the result in a new column named `clean_tokens`."]},{"cell_type":"markdown","metadata":{},"source":["`Tokenization is a way of separating a piece of text into smaller units called tokens. Tokens can be either words, characters, or subwords.`"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:24:55.173998Z","iopub.status.busy":"2021-10-10T09:24:55.173745Z","iopub.status.idle":"2021-10-10T09:25:09.258369Z","shell.execute_reply":"2021-10-10T09:25:09.257438Z","shell.execute_reply.started":"2021-10-10T09:24:55.173972Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>description</th>\n","      <th>clean_description</th>\n","      <th>clean_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Active classic boxers - There's a reason why o...</td>\n","      <td>active classic boxers - there's a reason why o...</td>\n","      <td>(active, classic, boxers, -, there, 's, a, rea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n","      <td>active sport boxer briefs - skinning up glory ...</td>\n","      <td>(active, sport, boxer, briefs, -, skinning, up...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Active sport briefs - These superbreathable no...</td>\n","      <td>active sport briefs - these superbreathable no...</td>\n","      <td>(active, sport, briefs, -, these, superbreatha...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n","      <td>alpine guide pants - skin in, climb ice, switc...</td>\n","      <td>(alpine, guide, pants, -, skin, in, ,, climb, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n","      <td>alpine wind jkt - on high ridges, steep ice an...</td>\n","      <td>(alpine, wind, jkt, -, on, high, ridges, ,, st...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                        description  \\\n","0   1  Active classic boxers - There's a reason why o...   \n","1   2  Active sport boxer briefs - Skinning up Glory ...   \n","2   3  Active sport briefs - These superbreathable no...   \n","3   4  Alpine guide pants - Skin in, climb ice, switc...   \n","4   5  Alpine wind jkt - On high ridges, steep ice an...   \n","\n","                                   clean_description  \\\n","0  active classic boxers - there's a reason why o...   \n","1  active sport boxer briefs - skinning up glory ...   \n","2  active sport briefs - these superbreathable no...   \n","3  alpine guide pants - skin in, climb ice, switc...   \n","4  alpine wind jkt - on high ridges, steep ice an...   \n","\n","                                        clean_tokens  \n","0  (active, classic, boxers, -, there, 's, a, rea...  \n","1  (active, sport, boxer, briefs, -, skinning, up...  \n","2  (active, sport, briefs, -, these, superbreatha...  \n","3  (alpine, guide, pants, -, skin, in, ,, climb, ...  \n","4  (alpine, wind, jkt, -, on, high, ridges, ,, st...  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["## Tokenize the cleaned description\n","corpus['clean_tokens'] = corpus['clean_description'].apply(lambda x: nlp(x))\n","corpus.head()"]},{"cell_type":"markdown","metadata":{},"source":["6. Removing the stop words and lemmatize `clean_tokens`"]},{"cell_type":"markdown","metadata":{},"source":["- `Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc.`\n","- `Lemmatization is a linguistic term that means grouping together words with the same root or lemma.`"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.260775Z","iopub.status.busy":"2021-10-10T09:25:09.260563Z","iopub.status.idle":"2021-10-10T09:25:09.392037Z","shell.execute_reply":"2021-10-10T09:25:09.391128Z","shell.execute_reply.started":"2021-10-10T09:25:09.260751Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>description</th>\n","      <th>clean_description</th>\n","      <th>clean_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Active classic boxers - There's a reason why o...</td>\n","      <td>active classic boxers - there's a reason why o...</td>\n","      <td>[active, classic, boxer, -, reason, boxer, cul...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n","      <td>active sport boxer briefs - skinning up glory ...</td>\n","      <td>[active, sport, boxer, brief, -, skin, glory, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Active sport briefs - These superbreathable no...</td>\n","      <td>active sport briefs - these superbreathable no...</td>\n","      <td>[active, sport, brief, -, superbreathable, -, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n","      <td>alpine guide pants - skin in, climb ice, switc...</td>\n","      <td>[alpine, guide, pant, -, skin, ,, climb, ice, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n","      <td>alpine wind jkt - on high ridges, steep ice an...</td>\n","      <td>[alpine, wind, jkt, -, high, ridge, ,, steep, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                        description  \\\n","0   1  Active classic boxers - There's a reason why o...   \n","1   2  Active sport boxer briefs - Skinning up Glory ...   \n","2   3  Active sport briefs - These superbreathable no...   \n","3   4  Alpine guide pants - Skin in, climb ice, switc...   \n","4   5  Alpine wind jkt - On high ridges, steep ice an...   \n","\n","                                   clean_description  \\\n","0  active classic boxers - there's a reason why o...   \n","1  active sport boxer briefs - skinning up glory ...   \n","2  active sport briefs - these superbreathable no...   \n","3  alpine guide pants - skin in, climb ice, switc...   \n","4  alpine wind jkt - on high ridges, steep ice an...   \n","\n","                                        clean_tokens  \n","0  [active, classic, boxer, -, reason, boxer, cul...  \n","1  [active, sport, boxer, brief, -, skin, glory, ...  \n","2  [active, sport, brief, -, superbreathable, -, ...  \n","3  [alpine, guide, pant, -, skin, ,, climb, ice, ...  \n","4  [alpine, wind, jkt, -, high, ridge, ,, steep, ...  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Remove stop words\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","corpus['clean_tokens'] = corpus['clean_tokens'].apply(lambda x: [token.lemma_ for token in x if token.text not in STOP_WORDS])\n","corpus.head()"]},{"cell_type":"markdown","metadata":{},"source":["7. Writing all the cleaned tokens into one single string and put it in a new column named `clean_document`."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.393418Z","iopub.status.busy":"2021-10-10T09:25:09.393187Z","iopub.status.idle":"2021-10-10T09:25:09.417668Z","shell.execute_reply":"2021-10-10T09:25:09.416821Z","shell.execute_reply.started":"2021-10-10T09:25:09.393391Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>description</th>\n","      <th>clean_description</th>\n","      <th>clean_tokens</th>\n","      <th>clean_document</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Active classic boxers - There's a reason why o...</td>\n","      <td>active classic boxers - there's a reason why o...</td>\n","      <td>[active, classic, boxer, -, reason, boxer, cul...</td>\n","      <td>active classic boxer - reason boxer cult favor...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Active sport boxer briefs - Skinning up Glory ...</td>\n","      <td>active sport boxer briefs - skinning up glory ...</td>\n","      <td>[active, sport, boxer, brief, -, skin, glory, ...</td>\n","      <td>active sport boxer brief - skin glory require ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Active sport briefs - These superbreathable no...</td>\n","      <td>active sport briefs - these superbreathable no...</td>\n","      <td>[active, sport, brief, -, superbreathable, -, ...</td>\n","      <td>active sport brief - superbreathable - fly bri...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Alpine guide pants - Skin in, climb ice, switc...</td>\n","      <td>alpine guide pants - skin in, climb ice, switc...</td>\n","      <td>[alpine, guide, pant, -, skin, ,, climb, ice, ...</td>\n","      <td>alpine guide pant - skin , climb ice , switch ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Alpine wind jkt - On high ridges, steep ice an...</td>\n","      <td>alpine wind jkt - on high ridges, steep ice an...</td>\n","      <td>[alpine, wind, jkt, -, high, ridge, ,, steep, ...</td>\n","      <td>alpine wind jkt - high ridge , steep ice alpin...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                        description  \\\n","0   1  Active classic boxers - There's a reason why o...   \n","1   2  Active sport boxer briefs - Skinning up Glory ...   \n","2   3  Active sport briefs - These superbreathable no...   \n","3   4  Alpine guide pants - Skin in, climb ice, switc...   \n","4   5  Alpine wind jkt - On high ridges, steep ice an...   \n","\n","                                   clean_description  \\\n","0  active classic boxers - there's a reason why o...   \n","1  active sport boxer briefs - skinning up glory ...   \n","2  active sport briefs - these superbreathable no...   \n","3  alpine guide pants - skin in, climb ice, switc...   \n","4  alpine wind jkt - on high ridges, steep ice an...   \n","\n","                                        clean_tokens  \\\n","0  [active, classic, boxer, -, reason, boxer, cul...   \n","1  [active, sport, boxer, brief, -, skin, glory, ...   \n","2  [active, sport, brief, -, superbreathable, -, ...   \n","3  [alpine, guide, pant, -, skin, ,, climb, ice, ...   \n","4  [alpine, wind, jkt, -, high, ridge, ,, steep, ...   \n","\n","                                      clean_document  \n","0  active classic boxer - reason boxer cult favor...  \n","1  active sport boxer brief - skin glory require ...  \n","2  active sport brief - superbreathable - fly bri...  \n","3  alpine guide pant - skin , climb ice , switch ...  \n","4  alpine wind jkt - high ridge , steep ice alpin...  "]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Put back tokens into one single string\n","corpus[\"clean_document\"] = [\" \".join(x) for x in corpus['clean_tokens']]\n","corpus.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Part 1 conclusion :\n","Now we have the `clean_document` feature which is cleaned. We can train some NLP model on it."]},{"cell_type":"markdown","metadata":{},"source":["## Part 2 - Clustering model --> Identify similar products <a class=\"anchor\" id=\"chapter2\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["1. TF-IDF transformation from the column `clean_document`"]},{"cell_type":"markdown","metadata":{},"source":["`TF-IDF (term frequency-inverse document frequency)` is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n","\n","This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.419021Z","iopub.status.busy":"2021-10-10T09:25:09.418757Z","iopub.status.idle":"2021-10-10T09:25:09.532086Z","shell.execute_reply":"2021-10-10T09:25:09.531467Z","shell.execute_reply.started":"2021-10-10T09:25:09.418993Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(500, 3930)\n"]}],"source":["# TF-IDF vector\n","vectorizer = TfidfVectorizer(stop_words='english')\n","X = vectorizer.fit_transform(corpus[\"clean_document\"])\n","\n","# X is a generator. We can transform that as an array\n","X = X.toarray()\n","print(X.shape)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.533703Z","iopub.status.busy":"2021-10-10T09:25:09.533049Z","iopub.status.idle":"2021-10-10T09:25:09.543064Z","shell.execute_reply":"2021-10-10T09:25:09.542205Z","shell.execute_reply.started":"2021-10-10T09:25:09.533669Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[('000', 0), ('03', 1), ('10', 2), ('100', 3), ('1000', 4), ('1021', 5), ('1027', 6), ('103', 7), ('1038', 8), ('1055', 9), ('106', 10), ('1070', 11), ('108', 12), ('109', 13), ('1096', 14), ('11', 15), ('110', 16), ('112', 17), ('1125', 18), ('1128', 19), ('1139', 20), ('115', 21), ('116', 22), ('1171', 23), ('118', 24), ('1188', 25), ('11c', 26), ('12', 27), ('1200', 28), ('121', 29), ('1234', 30), ('124', 31), ('125', 32), ('126', 33), ('127', 34), ('129', 35), ('1298', 36), ('12d', 37), ('13', 38), ('130', 39), ('132', 40), ('1324', 41), ('1327', 42), ('133', 43), ('1341', 44), ('135', 45), ('138', 46), ('14', 47), ('141', 48), ('144', 49)]\n"]}],"source":["# Print the 50 first words into our vocabulary\n","print(sorted(vectorizer.vocabulary_.items())[:50])"]},{"cell_type":"markdown","metadata":{},"source":["2. Create a DataFrame containing the result from TF-IDF"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.545531Z","iopub.status.busy":"2021-10-10T09:25:09.544902Z","iopub.status.idle":"2021-10-10T09:25:09.587224Z","shell.execute_reply":"2021-10-10T09:25:09.586361Z","shell.execute_reply.started":"2021-10-10T09:25:09.545489Z"},"trusted":true},"outputs":[{"ename":"AttributeError","evalue":"'TfidfVectorizer' object has no attribute 'get_feature_names'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a dataframe with tf-idf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X, \n\u001b[1;32m----> 3\u001b[0m              columns\u001b[38;5;241m=\u001b[39m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m(), \n\u001b[0;32m      4\u001b[0m              index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(corpus\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])] )\n\u001b[0;32m      6\u001b[0m X_df\u001b[38;5;241m.\u001b[39mhead()\n","\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"]}],"source":["# Create a dataframe with tf-idf\n","X_df = pd.DataFrame(X, \n","             columns=vectorizer.get_feature_names(), \n","             index=[\"item_{}\".format(x) for x in range(corpus.shape[0])] )\n","\n","X_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["- Each line of X correspond to a product description.\n","- Each column of X correspond to a word into the vocabulary.\n","- So each cell of X correspond to the score TF-IDF for a word into a product description."]},{"cell_type":"markdown","metadata":{},"source":["3. Using DBSCAN to make some clustering on the TF-IDF matrix. "]},{"cell_type":"markdown","metadata":{},"source":["- When dealing with texts, the distance metric to be used is `cosine` instead of \"euclidean\". \n","- eps = 0.7 and min samples = 3 enable to have almost 15 clusters without too much outliers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.589029Z","iopub.status.busy":"2021-10-10T09:25:09.588697Z","iopub.status.idle":"2021-10-10T09:25:09.737473Z","shell.execute_reply":"2021-10-10T09:25:09.736432Z","shell.execute_reply.started":"2021-10-10T09:25:09.58899Z"},"trusted":true},"outputs":[],"source":["# Clustering on documents with DBSCAN\n","clustering = DBSCAN(eps=0.7, min_samples=3, metric=\"cosine\", algorithm=\"brute\")\n","\n","# Fit on data \n","#No need to normalize data, it already is due to TF-IDF\n","clustering.fit(X)\n","\n","# Write cluster ids into corpus and X_df\n","corpus['cluster_id'] = clustering.labels_\n","display(corpus.head())\n","X_df['cluster_id'] = clustering.labels_\n","display(X_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["4. Display number of documents in each cluster"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.739699Z","iopub.status.busy":"2021-10-10T09:25:09.739133Z","iopub.status.idle":"2021-10-10T09:25:09.750913Z","shell.execute_reply":"2021-10-10T09:25:09.749915Z","shell.execute_reply.started":"2021-10-10T09:25:09.739649Z"},"trusted":true},"outputs":[],"source":["# Number of documents in each cluster\n","corpus['cluster_id'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["5. Print a sample of 3 documents that belong to 5 clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.75363Z","iopub.status.busy":"2021-10-10T09:25:09.75279Z","iopub.status.idle":"2021-10-10T09:25:09.775386Z","shell.execute_reply":"2021-10-10T09:25:09.771154Z","shell.execute_reply.started":"2021-10-10T09:25:09.753576Z"},"trusted":true},"outputs":[],"source":["# Print sample of 3 documents for the 5 first cluster\n","for c in corpus['cluster_id'].value_counts().index[:5] :\n","    print(\"CLUSTER \", c , ' :')\n","    print('----')\n","    for d in corpus.loc[corpus['cluster_id']==c,:].sample(3)['clean_description']:\n","        print(d)\n","        print()\n","    print('-----------')"]},{"cell_type":"markdown","metadata":{},"source":["6. Print the 5 most frequent words in the 5 first clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.778269Z","iopub.status.busy":"2021-10-10T09:25:09.777844Z","iopub.status.idle":"2021-10-10T09:25:09.88972Z","shell.execute_reply":"2021-10-10T09:25:09.8887Z","shell.execute_reply.started":"2021-10-10T09:25:09.778227Z"},"trusted":true},"outputs":[],"source":["# 5 Most frequent words in each cluster\n","cols = [c for c in X_df.columns if c!='cluster_id']\n","\n","for c in corpus['cluster_id'].value_counts().index[:5] :\n","    print(\"CLUSTER \", c)\n","    print(X_df.loc[X_df['cluster_id']==c,cols].mean(axis=0).sort_values(ascending=False)[0:5])\n","    print('-----------')"]},{"cell_type":"markdown","metadata":{},"source":["7. Wordcloud for the 5 first clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:09.893636Z","iopub.status.busy":"2021-10-10T09:25:09.893404Z","iopub.status.idle":"2021-10-10T09:25:13.085762Z","shell.execute_reply":"2021-10-10T09:25:13.084936Z","shell.execute_reply.started":"2021-10-10T09:25:09.893609Z"},"trusted":true},"outputs":[],"source":["# Word cloud for the 5 first clusters\n","wd = wordcloud.WordCloud()\n","for c in corpus['cluster_id'].value_counts().index[:5] :\n","    print(\"CLUSTER \", c)\n","    texts = \" \".join(corpus.loc[corpus['cluster_id']==c,'clean_description'])\n","    cloud = wd.generate(texts)\n","    plt.imshow(cloud)\n","    plt.show()\n","    print('-----------')"]},{"cell_type":"markdown","metadata":{},"source":["## Part 3 - Recommender system <a class=\"anchor\" id=\"chapter3\"></a>\n","\n","Now, we use the clusters created from part 1 to build a recommender system. \n","The aim is to be able to suggest to a user some products that are similar to the ones he is interested in. To do this, we consider that products belonging to the same cluster are similar."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:13.087195Z","iopub.status.busy":"2021-10-10T09:25:13.086982Z","iopub.status.idle":"2021-10-10T09:25:13.106833Z","shell.execute_reply":"2021-10-10T09:25:13.105712Z","shell.execute_reply.started":"2021-10-10T09:25:13.08717Z"},"trusted":true},"outputs":[],"source":["corpus.head()"]},{"cell_type":"markdown","metadata":{},"source":["1. We Create a function named `find_similar_items` that return the 5 items ids belonging to the same cluster as the product `item_id` which is passed in arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:13.108825Z","iopub.status.busy":"2021-10-10T09:25:13.108414Z","iopub.status.idle":"2021-10-10T09:25:13.117749Z","shell.execute_reply":"2021-10-10T09:25:13.117016Z","shell.execute_reply.started":"2021-10-10T09:25:13.10878Z"},"trusted":true},"outputs":[],"source":["def find_similar_items(item_id):\n","    \"\"\"\n","    Return 5 product ids belonging to the same cluster as item_id\n","    \"\"\"\n","    cluster_id = corpus.loc[corpus['id']==item_id, 'cluster_id'].values[0]\n","    similar_items = corpus.loc[corpus['cluster_id']==cluster_id,:].sample(5)\n","    similar_item_ids = similar_items['id'].unique()\n","    return similar_item_ids"]},{"cell_type":"markdown","metadata":{},"source":["2. Using python's `input()` function to allow the user to choose a product and submit some suggestions of similar items"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:13.119219Z","iopub.status.busy":"2021-10-10T09:25:13.118882Z","iopub.status.idle":"2021-10-10T09:25:13.133294Z","shell.execute_reply":"2021-10-10T09:25:13.132202Z","shell.execute_reply.started":"2021-10-10T09:25:13.119191Z"},"trusted":true},"outputs":[],"source":["# For printing in colors\n","class bcolors:\n","    OKBLUE = '\\033[94m'\n","    OKGREEN = '\\033[92m'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:13.135248Z","iopub.status.busy":"2021-10-10T09:25:13.134807Z","iopub.status.idle":"2021-10-10T09:25:16.816176Z","shell.execute_reply":"2021-10-10T09:25:16.81516Z","shell.execute_reply.started":"2021-10-10T09:25:13.135206Z"},"trusted":true},"outputs":[],"source":["product_id = int(input(\"What product would you like to buy ? \"))\n","print()\n","try:\n","    item_desc = corpus.loc[corpus['id']==product_id, 'clean_description'].values[0]\n","except:\n","    print('Product not found in database. Please enter a valid product id.')\n","else:\n","    print(f\"{bcolors.OKBLUE}Product found in database, description below :\")\n","    print(item_desc)\n","    print()\n","    \n","    print(\"Based on the analysis of the products' descriptions, you might also be interested by the following products : \")\n","    print()\n","\n","    for i in find_similar_items(product_id):\n","        print(f\"{bcolors.OKGREEN}Item #\", i)\n","        print(corpus.loc[corpus['id']==i, 'clean_description'].values[0])\n","        print('--------------------')"]},{"cell_type":"markdown","metadata":{},"source":["## Part 4 - LSA for topic extraction <a class=\"anchor\" id=\"chapter4\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["- Latent Semantic Analysis, or LSA, is one of the foundational techniques in topic modeling. The core idea is to take a matrix of what we have — documents and terms — and decompose it into a separate document-topic matrix and a topic-term matrix.\n","\n","- The main goal of this part is to find relevant topics for each documents. Contrary to clustering, a unique documents can have multiple topics. For example Sport and Politics."]},{"cell_type":"markdown","metadata":{},"source":["1. Using TruncatedSVD to make some topic extraction from the TF-IDF matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:16.81819Z","iopub.status.busy":"2021-10-10T09:25:16.817878Z","iopub.status.idle":"2021-10-10T09:25:16.948052Z","shell.execute_reply":"2021-10-10T09:25:16.947183Z","shell.execute_reply.started":"2021-10-10T09:25:16.818149Z"},"trusted":true},"outputs":[],"source":["# Train SVD model\n","svd_model = TruncatedSVD(n_components=12) # We test on 12 topics\n","lsa = svd_model.fit_transform(X)\n","topic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_\" + str(i) for i in range(lsa.shape[1])])\n","topic_encoded_df[\"documents\"] = corpus['clean_description']\n","topic_encoded_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Contrary to clustering, LSA allows to map each document to a mixing of several topics. For this reason, it's a bit more difficult to interpret the topics as categories : one document can actually be related to several topics at a time. To make things easier, we can extract the main topic of each document."]},{"cell_type":"markdown","metadata":{},"source":["2. Creation of a new column named `main_topic` in `topic_encoded_df` where we store the main topics related to each document"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:16.9502Z","iopub.status.busy":"2021-10-10T09:25:16.949682Z","iopub.status.idle":"2021-10-10T09:25:17.26944Z","shell.execute_reply":"2021-10-10T09:25:17.268914Z","shell.execute_reply.started":"2021-10-10T09:25:16.950156Z"},"trusted":true},"outputs":[],"source":["def extract_main_topics(x):\n","    \"\"\"\n","    Return the main topic for each document. The main topic is that have the maximum value for each line\n","    \"\"\"\n","    topics = np.abs(x)\n","    main_topic = topics.sort_values(ascending=False).index[0]\n","    return main_topic\n","\n","# Initialize column main_topics with 0\n","topic_encoded_df.loc[:, 'main_topic'] = 0\n","\n","for i, row in topic_encoded_df.iloc[:,:-2].iterrows():\n","    topic_encoded_df.loc[i, 'main_topic'] = extract_main_topics(row)\n","\n","topic_encoded_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["3. Couting each main topic in the corpus "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:17.270689Z","iopub.status.busy":"2021-10-10T09:25:17.27039Z","iopub.status.idle":"2021-10-10T09:25:17.278718Z","shell.execute_reply":"2021-10-10T09:25:17.27794Z","shell.execute_reply.started":"2021-10-10T09:25:17.270664Z"},"trusted":true},"outputs":[],"source":["topic_encoded_df['main_topic'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["- topic_0 is the most represented topic, as it's the main topic for more than `50%` of the documents of the corpus."]},{"cell_type":"markdown","metadata":{},"source":["4. Using the attribute `components_` of the SVD model to print the 5 most important words in each topic"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:17.280486Z","iopub.status.busy":"2021-10-10T09:25:17.280037Z","iopub.status.idle":"2021-10-10T09:25:17.312095Z","shell.execute_reply":"2021-10-10T09:25:17.311449Z","shell.execute_reply.started":"2021-10-10T09:25:17.280446Z"},"trusted":true},"outputs":[],"source":["# Create DataFrame containing the description of each topic in terms of the words in the vocabulary\n","topics_description = pd.DataFrame(svd_model.components_, columns = vectorizer.get_feature_names(), \n","                                  index = ['topic_' + str(i) for i in range(svd_model.components_.shape[0])])\n","\n","# Compute absolute values of coefficients\n","topics_description = topics_description.apply(np.abs, axis = 1)\n","\n","# Each word is map with a score of relevance for each topic\n","topics_description.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:17.313393Z","iopub.status.busy":"2021-10-10T09:25:17.313081Z","iopub.status.idle":"2021-10-10T09:25:17.339036Z","shell.execute_reply":"2021-10-10T09:25:17.338169Z","shell.execute_reply.started":"2021-10-10T09:25:17.313367Z"},"trusted":true},"outputs":[],"source":["# Loop over each topic and print the 5 most important words\n","for i,row in topics_description.iterrows():\n","    print('TOPIC :', i)\n","    print(row.sort_values(ascending=False)[0:5].index.tolist())\n","    print()\n","    print('-------------------------')\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["5. Make a wordcloud describing each topic and compare to the ones we obtain with clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:17.340441Z","iopub.status.busy":"2021-10-10T09:25:17.340198Z","iopub.status.idle":"2021-10-10T09:25:17.361961Z","shell.execute_reply":"2021-10-10T09:25:17.360935Z","shell.execute_reply.started":"2021-10-10T09:25:17.340414Z"},"trusted":true},"outputs":[],"source":["topic_encoded_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-10T09:25:17.363658Z","iopub.status.busy":"2021-10-10T09:25:17.363363Z","iopub.status.idle":"2021-10-10T09:25:24.688245Z","shell.execute_reply":"2021-10-10T09:25:24.687306Z","shell.execute_reply.started":"2021-10-10T09:25:17.363627Z"},"trusted":true},"outputs":[],"source":["# Loop over each topic and create wordcloud from documents that are related to this main topic\n","wd = wordcloud.WordCloud()\n","\n","cols = [c for c in topic_encoded_df.columns if 'topic_' in c]\n","\n","for t in cols:\n","    print('-------------------------')\n","    print()\n","    print('TOPIC ', t)\n","    \n","    # Handle topics that are not main topics for any document in the corpus\n","    if (topic_encoded_df['main_topic']==t).any() == False :\n","        print('cannot create wordcloud for this topic')\n","        continue\n","    \n","    texts = \" \".join(topic_encoded_df.loc[topic_encoded_df['main_topic']==t,'documents'])\n","    cloud = wd.generate(texts)\n","    plt.imshow(cloud)\n","    plt.show()\n","    \n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion\n","\n","- It's difficult to compare the results from the clustering and LSA, in particular because we didn't get exactly the same number of \"topics\" for both algorithms. However, the wordclouds are not fundamentally different (for example, both algorithms identify a group of documents or topic related to sun protection, organic cotton or merino wood).\n","\n","- The major difference between these two approaches is that clustering maps a given document to a single group, whereas LSA links a document to several topics. For this reason, text clustering is usually more suitable for applications related to similarity measurements (for example, building a recommender system),  whereas LSA is widely used for topic modelling."]},{"cell_type":"markdown","metadata":{},"source":["#### If you like this botebook, please up-vote ☝️\n","#### It will help 🙂"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
